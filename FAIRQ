import numpy as np

class FairQAgent:
    def __init__(self, state_size, action_size, alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.99):
        self.state_size = state_size
        self.action_size = action_size
        self.alpha = alpha  # Learning rate
        self.gamma = gamma  # Discount factor
        self.epsilon = epsilon  # Exploration rate
        self.epsilon_decay = epsilon_decay
        self.q_table = np.zeros((state_size, action_size))

    def choose_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.choice(self.action_size)  # Explore
        else:
            return np.argmax(self.q_table[state])  # Exploit

    def learn(self, state, action, reward, next_state):
        best_next_action = np.argmax(self.q_table[next_state])
        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]
        td_error = td_target - self.q_table[state, action]
        self.q_table[state, action] += self.alpha * td_error

        # Decay epsilon
        if self.epsilon > 0.01:
            self.epsilon *= self.epsilon_decay

    def update_q_table(self, state, action, reward, next_state):
        self.learn(state, action, reward, next_state)
        return self.q_table

# Example usage
state_size = 5  # Number of states
action_size = 2  # Number of actions

agent = FairQAgent(state_size, action_size)

# Example of updating the Q-table
state = 0
action = agent.choose_action(state)
next_state = 1
reward = 1  # Example reward
agent.update_q_table(state, action, reward, next_state)

print("Updated Q-table:")
print(agent.q_table)
